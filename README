This is a general purpose sparse optimizer to solve data fitting problems. It tries to find the
vector p that minimizes

 norm2( x )

where x = f(p) is a vector that has higher dimensionality than p. The user passes in a callback
function that takes in the vector p and returns the vector x and a matrix of derivatives J =
df/dp. J is a matrix with a row for each element of f and a column for each element of p. J is a
sparse matrix, which results in substantial increases in computational efficiency if most entries of
J are 0. J is stored row-first in the callback routine. libdogleg uses a column-first data
representation so it references the transpose of J (called Jt). J stored row-first is identical to
Jt stored column-first; this is purely a naming choice.

This library implements Powell's dog-leg algorithm to solve the problem. Like the more-widely-known
Levenberg-Marquardt algorithm, Powell's dog-leg algorithm solves a nonlinear optimization problem by
interpolating between a Gauss-Newton step and a gradient descent step. Improvements over LM are

 - a more natural representation of the linearity of the operating point (trust region size vs a vague
   lambda term).
 - significant efficiency gains, since a matrix inversion isn't needed to retry a rejected step

The algorithm is described in many places, originally in

M. Powell. A Hybrid Method for Nonlinear Equations. In P. Rabinowitz, editor, Numerical Methods for
Nonlinear Algebraic Equations, pages 87Â­144.  Gordon and Breach Science, London, 1970.

Various enhancements to Powell's original method are described in the literature; at this time only
the original algorithm is implemented here. 

The sparse matrix algebra is handled by the CHOLMOD library, written by Tim Davis. Parts of CHOLMOD
are licensed under the GPL and parts under the LGPL. Only the LGPL pieces are used here, allowing
libdogleg to be licensed under the LGPL as well. Due to this I lose some convenience (all simple
sparse matrix arithmetic in CHOLMOD is GPL-ed) and some performance (the fancier computational
methods, such as supernodal analysis are GPL-ed). For my current applications the performance losses
are minor.


Areas to improve

The current implementation doesn't handle a singular JtJ gracefully. Analytically, JtJ is at worst
positive semi-definite (has 0 eigenvalues). If a singular JtJ is ever encountered, from that point
on, JtJ + lambda*I is inverted instead for some positive constant lambda. This makes the matrix
strictly positive definite, but is sloppy. At least I should vary lambda. In my current
applications, a singular JtJ only occurs if at a particular operating point the vector x has no
dependence at all on some elements of p. In the general case other causes could exist, though.

There's an inefficiency in that the callback always returns x and J. When I evaluate and reject a
step, I do not end up using J at all. Dependng on the callback function, it may be better to ask for
x and then, if the step is accepted, to ask for J.
